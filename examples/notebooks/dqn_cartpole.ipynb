{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from deeprl.agents.dqn import DQNAgent\n",
    "from deeprl.core import Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-01 01:45:01,948] Making new env: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=state_size, activation='elu'))\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(action_size, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CartpoleProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return observation.reshape((1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(model, processor=CartpoleProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=100/100000: episode_reward=9.0\n",
      "episode=200/100000: episode_reward=9.0\n",
      "episode=300/100000: episode_reward=11.0\n",
      "episode=400/100000: episode_reward=10.0\n",
      "episode=500/100000: episode_reward=8.0\n",
      "episode=600/100000: episode_reward=9.0\n",
      "episode=700/100000: episode_reward=9.0\n",
      "episode=800/100000: episode_reward=12.0\n",
      "episode=900/100000: episode_reward=10.0\n",
      "episode=1000/100000: episode_reward=10.0\n",
      "episode=1100/100000: episode_reward=9.0\n",
      "episode=1200/100000: episode_reward=9.0\n",
      "episode=1300/100000: episode_reward=8.0\n",
      "episode=1400/100000: episode_reward=10.0\n",
      "episode=1500/100000: episode_reward=10.0\n",
      "episode=1600/100000: episode_reward=10.0\n",
      "episode=1700/100000: episode_reward=10.0\n",
      "episode=1800/100000: episode_reward=9.0\n",
      "episode=1900/100000: episode_reward=9.0\n",
      "episode=2000/100000: episode_reward=8.0\n",
      "episode=2100/100000: episode_reward=10.0\n",
      "episode=2200/100000: episode_reward=11.0\n",
      "episode=2300/100000: episode_reward=10.0\n",
      "episode=2400/100000: episode_reward=9.0\n",
      "episode=2500/100000: episode_reward=10.0\n",
      "episode=2600/100000: episode_reward=8.0\n",
      "episode=2700/100000: episode_reward=9.0\n",
      "episode=2800/100000: episode_reward=10.0\n",
      "episode=2900/100000: episode_reward=9.0\n",
      "episode=3000/100000: episode_reward=10.0\n",
      "episode=3100/100000: episode_reward=9.0\n",
      "episode=3200/100000: episode_reward=12.0\n",
      "episode=3300/100000: episode_reward=10.0\n",
      "episode=3400/100000: episode_reward=11.0\n",
      "episode=3500/100000: episode_reward=9.0\n",
      "episode=3600/100000: episode_reward=11.0\n",
      "episode=3700/100000: episode_reward=10.0\n",
      "episode=3800/100000: episode_reward=8.0\n",
      "episode=3900/100000: episode_reward=11.0\n",
      "episode=4000/100000: episode_reward=10.0\n",
      "episode=4100/100000: episode_reward=9.0\n",
      "episode=4200/100000: episode_reward=10.0\n",
      "episode=4300/100000: episode_reward=9.0\n",
      "episode=4400/100000: episode_reward=11.0\n",
      "episode=4500/100000: episode_reward=9.0\n",
      "episode=4600/100000: episode_reward=9.0\n",
      "episode=4700/100000: episode_reward=9.0\n",
      "episode=4800/100000: episode_reward=9.0\n",
      "episode=4900/100000: episode_reward=12.0\n",
      "episode=5000/100000: episode_reward=9.0\n",
      "episode=5100/100000: episode_reward=9.0\n",
      "episode=5200/100000: episode_reward=35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-01 01:48:37,322] Agent training has been manually aborted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent training completed in 215.207032919 sec (aborted).\n"
     ]
    }
   ],
   "source": [
    "history = agent.train(env, n_episodes=100000, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
