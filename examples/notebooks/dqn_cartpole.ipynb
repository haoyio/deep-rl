{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from deeprl.agents.dqn import DQNAgent\n",
    "from deeprl.core import Processor\n",
    "from deeprl.memory import Memory\n",
    "from deeprl.policy import EpsilonGreedyPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-01 01:11:04,813] Making new env: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=state_size, activation='elu'))\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(action_size, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CartpoleProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return observation.reshape((1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(model, processor=CartpoleProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=1/100: episode_reward=16.0\n",
      "episode=2/100: episode_reward=22.0\n",
      "episode=3/100: episode_reward=17.0\n",
      "episode=4/100: episode_reward=22.0\n",
      "episode=5/100: episode_reward=34.0\n",
      "episode=6/100: episode_reward=33.0\n",
      "episode=7/100: episode_reward=27.0\n",
      "episode=8/100: episode_reward=34.0\n",
      "episode=9/100: episode_reward=49.0\n",
      "episode=10/100: episode_reward=29.0\n",
      "episode=11/100: episode_reward=37.0\n",
      "episode=12/100: episode_reward=94.0\n",
      "episode=13/100: episode_reward=38.0\n",
      "episode=14/100: episode_reward=27.0\n",
      "episode=15/100: episode_reward=22.0\n",
      "episode=16/100: episode_reward=16.0\n",
      "episode=17/100: episode_reward=30.0\n",
      "episode=18/100: episode_reward=41.0\n",
      "episode=19/100: episode_reward=19.0\n",
      "episode=20/100: episode_reward=22.0\n",
      "episode=21/100: episode_reward=28.0\n",
      "episode=22/100: episode_reward=29.0\n",
      "episode=23/100: episode_reward=21.0\n",
      "episode=24/100: episode_reward=37.0\n",
      "episode=25/100: episode_reward=87.0\n",
      "episode=26/100: episode_reward=46.0\n",
      "episode=27/100: episode_reward=28.0\n",
      "episode=28/100: episode_reward=21.0\n",
      "episode=29/100: episode_reward=28.0\n",
      "episode=30/100: episode_reward=39.0\n",
      "episode=31/100: episode_reward=47.0\n",
      "episode=32/100: episode_reward=20.0\n",
      "episode=33/100: episode_reward=24.0\n",
      "episode=34/100: episode_reward=27.0\n",
      "episode=35/100: episode_reward=23.0\n",
      "episode=36/100: episode_reward=41.0\n",
      "episode=37/100: episode_reward=25.0\n",
      "episode=38/100: episode_reward=27.0\n",
      "episode=39/100: episode_reward=22.0\n",
      "episode=40/100: episode_reward=24.0\n",
      "episode=41/100: episode_reward=79.0\n",
      "episode=42/100: episode_reward=27.0\n",
      "episode=43/100: episode_reward=34.0\n",
      "episode=44/100: episode_reward=23.0\n",
      "episode=45/100: episode_reward=23.0\n",
      "episode=46/100: episode_reward=29.0\n",
      "episode=47/100: episode_reward=27.0\n",
      "episode=48/100: episode_reward=53.0\n",
      "episode=49/100: episode_reward=37.0\n",
      "episode=50/100: episode_reward=31.0\n",
      "episode=51/100: episode_reward=23.0\n",
      "episode=52/100: episode_reward=24.0\n",
      "episode=53/100: episode_reward=27.0\n",
      "episode=54/100: episode_reward=29.0\n",
      "episode=55/100: episode_reward=35.0\n",
      "episode=56/100: episode_reward=43.0\n",
      "episode=57/100: episode_reward=34.0\n",
      "episode=58/100: episode_reward=15.0\n",
      "episode=59/100: episode_reward=20.0\n",
      "episode=60/100: episode_reward=33.0\n",
      "episode=61/100: episode_reward=41.0\n",
      "episode=62/100: episode_reward=35.0\n",
      "episode=63/100: episode_reward=13.0\n",
      "episode=64/100: episode_reward=42.0\n",
      "episode=65/100: episode_reward=68.0\n",
      "episode=66/100: episode_reward=23.0\n",
      "episode=67/100: episode_reward=23.0\n",
      "episode=68/100: episode_reward=17.0\n",
      "episode=69/100: episode_reward=28.0\n",
      "episode=70/100: episode_reward=23.0\n",
      "episode=71/100: episode_reward=37.0\n",
      "episode=72/100: episode_reward=26.0\n",
      "episode=73/100: episode_reward=37.0\n",
      "episode=74/100: episode_reward=22.0\n",
      "episode=75/100: episode_reward=32.0\n",
      "episode=76/100: episode_reward=19.0\n",
      "episode=77/100: episode_reward=26.0\n",
      "episode=78/100: episode_reward=33.0\n",
      "episode=79/100: episode_reward=20.0\n",
      "episode=80/100: episode_reward=19.0\n",
      "episode=81/100: episode_reward=60.0\n",
      "episode=82/100: episode_reward=79.0\n",
      "episode=83/100: episode_reward=33.0\n",
      "episode=84/100: episode_reward=20.0\n",
      "episode=85/100: episode_reward=34.0\n",
      "episode=86/100: episode_reward=27.0\n",
      "episode=87/100: episode_reward=16.0\n",
      "episode=88/100: episode_reward=37.0\n",
      "episode=89/100: episode_reward=15.0\n",
      "episode=90/100: episode_reward=34.0\n",
      "episode=91/100: episode_reward=52.0\n",
      "episode=92/100: episode_reward=45.0\n",
      "episode=93/100: episode_reward=34.0\n",
      "episode=94/100: episode_reward=46.0\n",
      "episode=95/100: episode_reward=23.0\n",
      "episode=96/100: episode_reward=16.0\n",
      "episode=97/100: episode_reward=47.0\n",
      "episode=98/100: episode_reward=23.0\n",
      "episode=99/100: episode_reward=29.0\n",
      "episode=100/100: episode_reward=17.0\n",
      "Agent training completed in 5.43439102173 sec.\n"
     ]
    }
   ],
   "source": [
    "history = agent.train(env, 100, max_episode_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
