{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from deeprl.agents.dqn import DQNAgent\n",
    "from deeprl.core import Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-01 03:09:00,252] Making new env: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=state_size, activation='elu'))\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(action_size, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CartpoleProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return observation.reshape((1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "    model, \n",
    "    memory_limit=100000,\n",
    "    min_experiences=1000,\n",
    "    processor=CartpoleProcessor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent initialization...\n",
      "Completed agent initialization in 1.80256009102 sec.\n",
      "Starting agent training...\n",
      "episode=1/200: episode_reward=12.0, avg_reward=42.6\n",
      "episode=2/200: episode_reward=10.0, avg_reward=40.1\n",
      "episode=3/200: episode_reward=43.0, avg_reward=34.9\n",
      "episode=4/200: episode_reward=21.0, avg_reward=21.8\n",
      "episode=5/200: episode_reward=22.0, avg_reward=23.3\n",
      "episode=6/200: episode_reward=19.0, avg_reward=20.9\n",
      "episode=7/200: episode_reward=39.0, avg_reward=43.7\n",
      "episode=8/200: episode_reward=49.0, avg_reward=53.1\n",
      "episode=9/200: episode_reward=95.0, avg_reward=49.7\n",
      "episode=10/200: episode_reward=23.0, avg_reward=49.1\n",
      "episode=11/200: episode_reward=86.0, avg_reward=38.1\n",
      "episode=12/200: episode_reward=39.0, avg_reward=40.0\n",
      "episode=13/200: episode_reward=61.0, avg_reward=58.1\n",
      "episode=14/200: episode_reward=36.0, avg_reward=45.4\n",
      "episode=15/200: episode_reward=57.0, avg_reward=45.0\n",
      "episode=16/200: episode_reward=54.0, avg_reward=67.3\n",
      "episode=17/200: episode_reward=45.0, avg_reward=39.2\n",
      "episode=18/200: episode_reward=59.0, avg_reward=61.0\n",
      "episode=19/200: episode_reward=44.0, avg_reward=58.9\n",
      "episode=20/200: episode_reward=60.0, avg_reward=51.2\n",
      "episode=21/200: episode_reward=45.0, avg_reward=53.4\n",
      "episode=22/200: episode_reward=66.0, avg_reward=48.1\n",
      "episode=23/200: episode_reward=29.0, avg_reward=49.2\n",
      "episode=24/200: episode_reward=96.0, avg_reward=56.8\n",
      "episode=25/200: episode_reward=44.0, avg_reward=40.6\n",
      "episode=26/200: episode_reward=74.0, avg_reward=85.8\n",
      "episode=27/200: episode_reward=117.0, avg_reward=65.9\n",
      "episode=28/200: episode_reward=47.0, avg_reward=43.9\n",
      "episode=29/200: episode_reward=36.0, avg_reward=61.3\n",
      "episode=30/200: episode_reward=45.0, avg_reward=51.5\n",
      "episode=31/200: episode_reward=56.0, avg_reward=42.9\n",
      "episode=32/200: episode_reward=50.0, avg_reward=45.2\n",
      "episode=33/200: episode_reward=34.0, avg_reward=60.3\n",
      "episode=34/200: episode_reward=34.0, avg_reward=69.1\n",
      "episode=35/200: episode_reward=65.0, avg_reward=67.0\n",
      "episode=36/200: episode_reward=108.0, avg_reward=88.2\n",
      "episode=37/200: episode_reward=99.0, avg_reward=159.4\n",
      "episode=38/200: episode_reward=148.0, avg_reward=200.9\n",
      "episode=39/200: episode_reward=219.0, avg_reward=239.7\n",
      "episode=40/200: episode_reward=116.0, avg_reward=486.5\n",
      "episode=41/200: episode_reward=303.0, avg_reward=500.0\n",
      "episode=42/200: episode_reward=500.0, avg_reward=500.0\n",
      "episode=43/200: episode_reward=500.0, avg_reward=500.0\n",
      "episode=44/200: episode_reward=500.0, avg_reward=494.2\n",
      "episode=45/200: episode_reward=485.0, avg_reward=500.0\n",
      "episode=46/200: episode_reward=267.0, avg_reward=475.6\n",
      "episode=47/200: episode_reward=354.0, avg_reward=493.6\n",
      "episode=48/200: episode_reward=269.0, avg_reward=453.3\n",
      "episode=49/200: episode_reward=439.0, avg_reward=443.2\n",
      "episode=50/200: episode_reward=358.0, avg_reward=415.9\n",
      "episode=51/200: episode_reward=382.0, avg_reward=273.2\n",
      "episode=52/200: episode_reward=244.0, avg_reward=234.5\n",
      "episode=53/200: episode_reward=241.0, avg_reward=436.1\n",
      "episode=54/200: episode_reward=345.0, avg_reward=387.4\n",
      "episode=55/200: episode_reward=269.0, avg_reward=376.6\n",
      "episode=56/200: episode_reward=214.0, avg_reward=298.4\n",
      "episode=57/200: episode_reward=244.0, avg_reward=371.7\n",
      "episode=58/200: episode_reward=262.0, avg_reward=353.1\n",
      "episode=59/200: episode_reward=362.0, avg_reward=335.5\n",
      "episode=60/200: episode_reward=311.0, avg_reward=379.1\n",
      "episode=61/200: episode_reward=399.0, avg_reward=289.9\n",
      "episode=62/200: episode_reward=276.0, avg_reward=364.2\n",
      "episode=63/200: episode_reward=292.0, avg_reward=272.8\n",
      "episode=64/200: episode_reward=237.0, avg_reward=271.1\n",
      "episode=65/200: episode_reward=230.0, avg_reward=323.4\n",
      "episode=66/200: episode_reward=380.0, avg_reward=428.4\n",
      "episode=67/200: episode_reward=294.0, avg_reward=355.1\n",
      "episode=68/200: episode_reward=478.0, avg_reward=364.9\n",
      "episode=69/200: episode_reward=277.0, avg_reward=349.6\n",
      "episode=70/200: episode_reward=288.0, avg_reward=256.8\n",
      "episode=71/200: episode_reward=256.0, avg_reward=256.8\n",
      "episode=72/200: episode_reward=322.0, avg_reward=397.1\n",
      "episode=73/200: episode_reward=305.0, avg_reward=384.0\n",
      "episode=74/200: episode_reward=357.0, avg_reward=379.2\n",
      "episode=75/200: episode_reward=304.0, avg_reward=440.9\n",
      "episode=76/200: episode_reward=500.0, avg_reward=474.9\n",
      "episode=77/200: episode_reward=352.0, avg_reward=473.8\n",
      "episode=78/200: episode_reward=319.0, avg_reward=403.5\n",
      "episode=79/200: episode_reward=334.0, avg_reward=414.4\n",
      "episode=80/200: episode_reward=320.0, avg_reward=309.8\n",
      "episode=81/200: episode_reward=353.0, avg_reward=396.5\n",
      "episode=82/200: episode_reward=330.0, avg_reward=395.8\n",
      "episode=83/200: episode_reward=249.0, avg_reward=247.0\n",
      "episode=84/200: episode_reward=191.0, avg_reward=294.3\n",
      "episode=85/200: episode_reward=210.0, avg_reward=449.3\n",
      "episode=86/200: episode_reward=235.0, avg_reward=234.5\n",
      "episode=87/200: episode_reward=284.0, avg_reward=269.0\n",
      "episode=88/200: episode_reward=255.0, avg_reward=320.5\n",
      "episode=89/200: episode_reward=219.0, avg_reward=238.1\n",
      "episode=90/200: episode_reward=228.0, avg_reward=242.8\n",
      "episode=91/200: episode_reward=258.0, avg_reward=216.6\n",
      "episode=92/200: episode_reward=225.0, avg_reward=219.2\n",
      "episode=93/200: episode_reward=258.0, avg_reward=215.2\n",
      "episode=94/200: episode_reward=199.0, avg_reward=413.1\n",
      "episode=95/200: episode_reward=423.0, avg_reward=247.1\n",
      "episode=96/200: episode_reward=400.0, avg_reward=308.1\n",
      "episode=97/200: episode_reward=209.0, avg_reward=330.9\n",
      "episode=98/200: episode_reward=328.0, avg_reward=314.6\n",
      "episode=99/200: episode_reward=271.0, avg_reward=258.2\n",
      "episode=100/200: episode_reward=294.0, avg_reward=216.2\n",
      "episode=101/200: episode_reward=212.0, avg_reward=202.4\n",
      "episode=102/200: episode_reward=223.0, avg_reward=237.4\n",
      "episode=103/200: episode_reward=263.0, avg_reward=271.2\n",
      "episode=104/200: episode_reward=224.0, avg_reward=355.7\n",
      "episode=105/200: episode_reward=282.0, avg_reward=276.2\n",
      "episode=106/200: episode_reward=253.0, avg_reward=305.8\n",
      "episode=107/200: episode_reward=270.0, avg_reward=231.1\n",
      "episode=108/200: episode_reward=239.0, avg_reward=235.8\n",
      "episode=109/200: episode_reward=245.0, avg_reward=306.1\n",
      "episode=110/200: episode_reward=254.0, avg_reward=374.5\n",
      "episode=111/200: episode_reward=267.0, avg_reward=302.0\n",
      "episode=112/200: episode_reward=231.0, avg_reward=320.5\n",
      "episode=113/200: episode_reward=321.0, avg_reward=313.6\n",
      "episode=114/200: episode_reward=232.0, avg_reward=259.7\n",
      "episode=115/200: episode_reward=286.0, avg_reward=240.2\n",
      "episode=116/200: episode_reward=216.0, avg_reward=320.0\n",
      "episode=117/200: episode_reward=412.0, avg_reward=240.6\n",
      "episode=118/200: episode_reward=274.0, avg_reward=277.4\n",
      "episode=119/200: episode_reward=227.0, avg_reward=262.1\n",
      "episode=120/200: episode_reward=301.0, avg_reward=250.4\n",
      "episode=121/200: episode_reward=256.0, avg_reward=256.2\n",
      "episode=122/200: episode_reward=274.0, avg_reward=301.5\n",
      "episode=123/200: episode_reward=248.0, avg_reward=300.2\n",
      "episode=124/200: episode_reward=399.0, avg_reward=382.9\n",
      "episode=125/200: episode_reward=289.0, avg_reward=421.2\n",
      "episode=126/200: episode_reward=331.0, avg_reward=429.5\n",
      "episode=127/200: episode_reward=361.0, avg_reward=317.0\n",
      "episode=128/200: episode_reward=375.0, avg_reward=402.8\n",
      "episode=129/200: episode_reward=500.0, avg_reward=467.3\n",
      "episode=130/200: episode_reward=500.0, avg_reward=458.0\n",
      "episode=131/200: episode_reward=500.0, avg_reward=409.7\n",
      "episode=132/200: episode_reward=330.0, avg_reward=288.6\n",
      "episode=133/200: episode_reward=243.0, avg_reward=242.5\n",
      "episode=134/200: episode_reward=218.0, avg_reward=321.9\n",
      "episode=135/200: episode_reward=500.0, avg_reward=295.6\n",
      "episode=136/200: episode_reward=299.0, avg_reward=257.0\n",
      "episode=137/200: episode_reward=285.0, avg_reward=264.3\n",
      "episode=138/200: episode_reward=308.0, avg_reward=237.8\n",
      "episode=139/200: episode_reward=292.0, avg_reward=265.6\n",
      "episode=140/200: episode_reward=232.0, avg_reward=441.0\n",
      "episode=141/200: episode_reward=304.0, avg_reward=325.3\n",
      "episode=142/200: episode_reward=214.0, avg_reward=333.5\n",
      "episode=143/200: episode_reward=299.0, avg_reward=251.8\n",
      "episode=144/200: episode_reward=352.0, avg_reward=313.8\n",
      "episode=145/200: episode_reward=274.0, avg_reward=292.6\n",
      "episode=146/200: episode_reward=333.0, avg_reward=327.2\n",
      "episode=147/200: episode_reward=314.0, avg_reward=392.9\n",
      "episode=148/200: episode_reward=396.0, avg_reward=266.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=149/200: episode_reward=275.0, avg_reward=223.6\n",
      "episode=150/200: episode_reward=316.0, avg_reward=429.8\n",
      "episode=151/200: episode_reward=313.0, avg_reward=235.6\n",
      "episode=152/200: episode_reward=276.0, avg_reward=413.5\n",
      "episode=153/200: episode_reward=500.0, avg_reward=390.3\n",
      "episode=154/200: episode_reward=350.0, avg_reward=387.6\n",
      "episode=155/200: episode_reward=361.0, avg_reward=397.2\n",
      "episode=156/200: episode_reward=294.0, avg_reward=423.8\n",
      "episode=157/200: episode_reward=456.0, avg_reward=349.2\n",
      "episode=158/200: episode_reward=293.0, avg_reward=377.8\n",
      "episode=159/200: episode_reward=403.0, avg_reward=382.7\n",
      "episode=160/200: episode_reward=324.0, avg_reward=301.8\n",
      "episode=161/200: episode_reward=286.0, avg_reward=228.8\n",
      "episode=162/200: episode_reward=225.0, avg_reward=260.7\n",
      "episode=163/200: episode_reward=346.0, avg_reward=236.0\n",
      "episode=164/200: episode_reward=316.0, avg_reward=213.1\n",
      "episode=165/200: episode_reward=175.0, avg_reward=10.5\n",
      "episode=166/200: episode_reward=10.0, avg_reward=10.5\n",
      "episode=167/200: episode_reward=13.0, avg_reward=12.5\n",
      "episode=168/200: episode_reward=12.0, avg_reward=16.6\n",
      "episode=169/200: episode_reward=200.0, avg_reward=344.1\n",
      "episode=170/200: episode_reward=215.0, avg_reward=321.9\n",
      "episode=171/200: episode_reward=437.0, avg_reward=265.2\n",
      "episode=172/200: episode_reward=253.0, avg_reward=315.0\n",
      "episode=173/200: episode_reward=462.0, avg_reward=257.3\n",
      "episode=174/200: episode_reward=270.0, avg_reward=354.1\n",
      "episode=175/200: episode_reward=347.0, avg_reward=390.2\n",
      "episode=176/200: episode_reward=371.0, avg_reward=442.4\n",
      "episode=177/200: episode_reward=474.0, avg_reward=451.5\n",
      "episode=178/200: episode_reward=447.0, avg_reward=422.7\n",
      "episode=179/200: episode_reward=430.0, avg_reward=407.6\n",
      "episode=180/200: episode_reward=500.0, avg_reward=372.9\n",
      "episode=181/200: episode_reward=278.0, avg_reward=415.8\n",
      "episode=182/200: episode_reward=437.0, avg_reward=364.0\n",
      "episode=183/200: episode_reward=500.0, avg_reward=353.6\n",
      "episode=184/200: episode_reward=440.0, avg_reward=376.1\n",
      "episode=185/200: episode_reward=271.0, avg_reward=377.4\n",
      "episode=186/200: episode_reward=500.0, avg_reward=308.3\n",
      "episode=187/200: episode_reward=264.0, avg_reward=437.9\n",
      "episode=188/200: episode_reward=500.0, avg_reward=410.2\n",
      "episode=189/200: episode_reward=283.0, avg_reward=299.0\n",
      "episode=190/200: episode_reward=500.0, avg_reward=366.9\n",
      "episode=191/200: episode_reward=500.0, avg_reward=480.9\n",
      "episode=192/200: episode_reward=357.0, avg_reward=380.8\n",
      "episode=193/200: episode_reward=500.0, avg_reward=451.9\n",
      "episode=194/200: episode_reward=500.0, avg_reward=475.2\n",
      "episode=195/200: episode_reward=500.0, avg_reward=381.2\n",
      "episode=196/200: episode_reward=408.0, avg_reward=390.1\n",
      "episode=197/200: episode_reward=348.0, avg_reward=448.6\n",
      "episode=198/200: episode_reward=500.0, avg_reward=410.6\n",
      "episode=199/200: episode_reward=342.0, avg_reward=313.8\n",
      "episode=200/200: episode_reward=448.0, avg_reward=497.9\n",
      "Completed agent training in 3647.03733301 sec.\n"
     ]
    }
   ],
   "source": [
    "history = agent.train(env, n_episodes=200, min_experiences=agent.min_experiences, n_simulations=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
